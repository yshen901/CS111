NAME: YUCI SHEN
EMAIL: SHEN.YUCI11@GMAIL.COM
UID: 604836772

FILES
-----
  * lab2_list.c		Source code for testing and implementing shared multi-list data structure manipulation
  * SortedList.h	Header file that describes the interface for cyclical doubly linked list operations
  * SortedList.c	Source code for cyclical doubly linked list operations

  * lab2b_list.csv	A CSV file containing all results from lab2_list tests
  * lab2b_list.gp	gnuplot script to produce graphs using lab2b_list.csv

  * lab2b_1.png	Graph: throughput vs. # of threads of mutex and spin-lock synchronized list operations
  * lab2b_2.png	Graph: wait-for-lock and average operation time vs. # of threads for mutex lock
  * lab2b_3.png	Graph: sucess/failure for iteration/# of thread combinations for no-sync, mutex, and spin-lock
  * lab2b_4.png	Graph: throughput vs. # of threads for varying # of lists for mutex
  * lab2b_5.png	Graph: throughput vs. # of threads for varying # of lists for spin-lock

  * Makefile		Supports 5 options: build (default)	builds the executables
    				   	    tests 		removes old CSV and collects test data by running lab2_list
					    graphs		runs tests then creates graphs using CSV data
					    dist		runs graphs then creates the tarball
					    clean		removes executable files
					    
  * README		Contains file descriptions and short response answers

QUESTIONS
---------
2.3.1) Cycles in the basic list implementation

       During the 1 and 2-thread list tests, most cycles are spent on list operations rather than on
       waiting/locking. This is because when the thread-count is low, there are fewer parallel operations
       executing on the same list and thus a lower chance of a thread needing to wait for a locked section
       to become unlocked.

       During the high-thread spin-lock tests, most cycles will be spent in on spinning and waiting for
       locks to release. This is because with higher number of threads, there is a higher chance of
       conflicts where multiple threads try to access the same piece of data, and are forced to wait.

       During the high-thread mutex tests, most cycles will be spent on locking and unlocking the mutex
       locks (mutex operations). These operations are very expensive, and more threads means more operations
       will be needed, and more time will be spent waiting.


2.3.2) Execution Profiling

       The lines of code that consume the most cycles in the spin-lock tests are:
       	   while(__sync_lock_test_and_set(s_lock + list, 1));

       There is only one line, as I moved the locking and unlocking operations into a separate function
       in order to keep my code clean. This is called to lock the thread during critical list operations.

       This operation becomes expensive with higher thread-counts because as mentioned above, more threads
       means a higher chance of multiple threads trying to execute on the same data structure. When this
       happens, threads will use up cycles spinning and waiting for their turn to access the locked list.


2.3.3) Mutex Wait Time

       The average lock-wait time rises so dramatically with the # of contending threads because more threads
       means a higher chance of multiple threads trying to execute on the same list. This increases the
       overall time spent waiting for resources to become unlocked, and increases wait time.

       The completion time per operation rises less dramatically with the number of contending threads
       because more threads are wasting cycles waiting for resources to unlock, but at the same time
       more threads are being productive and executing operations so the rise is less dramatic.

       Another reason for wait time to go up faster than completion time because wait time is often
       double counted, where threads waiting in the same "line" all add to the timer, as each thread
       has its own counter for wait time. This does not exist for completion time. 
   

2.3.4) Performance of Partitioned Lists

       Performance for both mutex and spin-lock seems to improve as the number of lists increase.
       This is due to the fact that a higher number of lists leads to more fine-grained synchronization,
       which leads to fewer cases of contention and less time spent waiting.

       Performance should continue to improve as the number of lists increase, and will hit a limit when
       each element has its own list.

       This suggestion seems to be true, and is backed up by the curves.
       Logically this is reasonable, as the chances of multiple threads trying to access the same
       resource are equal in both cases. 